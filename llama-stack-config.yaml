## LLAMA-STACK Distribution Descriptor
version: '2'
image_name: llamachat-backend

# Enabled APIs
apis:
- agents
- datasetio
- inference
- safety
- files
- telemetry
- tool_runtime
- vector_io

# Provider Configuration
providers:

  # Inference Providers
  inference:
    - provider_id: ollama
      provider_type: remote::ollama
      config:
        url: ${env.INFERENCE_URL:=http://localhost:11434}
    - provider_id: sentence-transformers
      provider_type: inline::sentence-transformers
      config: {}

  # Vector IO Providers
  vector_io:
    - provider_id: inline-milvus
      provider_type: inline::milvus
      config:
        db_path: ${env.MILVUS_DB_PATH:=~/.llama/distributions/llamachat-backend}/milvus.db
        kvstore:
          namespace: null
          type: sqlite
          db_path: ${env.MILVUS_KVSTORE_PATH:=~/.llama/distributions/llamachat-backend}/milvus_kvstore.db
    - provider_id: inline-faiss
      provider_type: inline::faiss
      config:
        kvstore:
          namespace: null
          type: sqlite
          db_path: ${env.FAISS_STORE_DIR:=~/.llama/distributions/llamachat-backend}/faiss_store.db

  # Safety Providers
  safety:
    - provider_id: llama-guard
      provider_type: inline::llama-guard
      config:
        excluded_categories: []

  # Files Backend
  files:
    - provider_type: inline::localfs
      provider_id: llama-localfs
      config:
        storage_dir: ${env.LOCAL_STORAGE_DIR:=~/.llama/distributions/llamachat-backend}/localfs
        metadata_store:
          type: sqlite
          db_path: ${env.LOCAL_STORAGE_DIR:=~/.llama/distributions/llamachat-backend/}/localfs/files_metadata.db

  # Agentic Backend Provider
  agents:
    - provider_id: meta-reference
      provider_type: inline::meta-reference
      config:
        persistence_store:
          type: sqlite
          namespace: null
          db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/llamachat-backend}/agents_store.db
        responses_store:
          type: sqlite
          db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/llamachat-backend}/responses_store.db

  # Dataset IO Providers
  datasetio:
    - provider_id: huggingface
      provider_type: remote::huggingface
      config:
        kvstore:
          type: sqlite
          namespace: null
          db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/llamachat-backend}/huggingface_datasetio.db
    - provider_id: localfs
      provider_type: inline::localfs
      config:
        kvstore:
          type: sqlite
          namespace: null
          db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/llamachat-backend}/localfs_datasetio.db

  telemetry:
    - provider_id: meta-reference
      provider_type: inline::meta-reference
      config:
        service_name: ${env.OTEL_SERVICE_NAME:=llama-stack}
        sinks: ${env.TELEMETRY_SINKS:=console,sqlite}
        otel_trace_endpoint: ${env.OTEL_TRACE_ENDPOINT:}
        sqlite_db_path: ${env.SQLITE_DB_PATH:=~/.llama/distributions/llamachat-backend/trace_store.db}

  tool_runtime:
  - provider_id: rag-runtime
    provider_type: inline::rag-runtime
    config: {}
  - provider_id: web-search
    provider_type: remote::bing-search
    config: {}
  - provider_id: model-context-protocol
    provider_type: remote::model-context-protocol
    config: {}

metadata_store:
  type: sqlite
  db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/llamachat-backend}/registry.db

inference_store:
  type: sqlite
  db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/llamachat-backend}/inference_store.db


## MODELS DEFINITION BY PROVIDER
models:
  - metadata: {}
    model_id: ${env.INFERENCE_MODEL:=qwen2.5-reasoning}
    provider_id: ollama
    provider_model_id: qwen2.5:7b
    model_type: llm
  - metadata: {}
    model_id: ${env.INFERENCE_MODEL:=qwen3-reasoning}
    provider_id: ollama
    provider_model_id: qwen3:8b
    model_type: llm
  - metadata: {}
    model_id: ${env.INFERENCE_MODEL:=gemma3n}
    provider_id: ollama
    provider_model_id: gemma3n:latest
    model_type: llm
  - metadata: {}
    model_id: ${env.INFERENCE_MODEL:=llama3-tool-calling}
    provider_id: ollama
    provider_model_id: llama3.2:3b
    model_type: llm
  - metadata: {}
    model_id: ${env.INFERENCE_MODEL:=llava-multimodal}
    provider_id: ollama
    provider_model_id: llava:7b
    model_type: llm
  - metadata:
      embedding_dimension: 384
    model_id: ${env.EMBEDDING_MODEL:=sentence-transformers}
    provider_id: sentence-transformers
    provider_model_id: all-MiniLM-L6-v2
    model_type: embedding
  - metadata:
      embedding_dimension: 3072
    model_id: ${env.EMBEDDING_MODEL:=llama-embed}
    provider_id: ollama
    provider_model_id: llama3.2:3b
    model_type: embedding
  - metadata: {}
    model_id: ${env.SAFETY_MODEL:=meta-llama/Llama-Guard-3-1B}
    provider_id: ollama
    provider_model_id: ${env.SAFETY_MODEL_OLLAMA:=llama-guard3:1b}
    model_type: llm

shields:
  - shield_id: ${env.SAFETY_MODEL:=meta-llama/Llama-Guard-3-1B}

vector_dbs: []
datasets: []
scoring_fns: []
benchmarks: []

# TOOLS
tool_groups:
  - toolgroup_id: builtin::rag
    provider_id: rag-runtime
  - toolgroup_id: builtin::websearch
    provider_id: web-search
  - toolgroup_id: mcp::tools
    provider_id: model-context-protocol
    mcp_endpoint:
      uri: "http://localhost:8000/sse"

# ENDPOINT SETTINGS
server:
  port: 8321
